{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "819c2039",
   "metadata": {
    "id": "4290f21d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import wget\n",
    "from pyspark.ml.feature import Bucketizer,RegexTokenizer,StopWordsRemover,CountVectorizer,IDF\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87ea614f",
   "metadata": {
    "id": "6d37dc26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f5d01655-b343-4dac-9196-8ebec5593e79;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.0.0 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.0/spark-sql-kafka-0-10_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0!spark-sql-kafka-0-10_2.12.jar (477ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.0.0/spark-avro_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.0.0!spark-avro_2.12.jar (236ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.0/mongo-spark-connector_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.0!mongo-spark-connector_2.12.jar (412ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.0.0/spark-token-provider-kafka-0-10_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0!spark-token-provider-kafka-0-10_2.12.jar (214ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.4.1!kafka-clients.jar (806ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (245ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (207ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.4-3/zstd-jni-1.4.4-3.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.4-3!zstd-jni.jar (988ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (315ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (532ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (214ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (234ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (304ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (488ms)\n",
      ":: resolution report :: resolve 24528ms :: artifacts dl 5719ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   14  |   14  |   0   ||   14  |   14  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f5d01655-b343-4dac-9196-8ebec5593e79\n",
      "\tconfs: [default]\n",
      "\t14 artifacts copied, 0 already retrieved (13279kB/66ms)\n",
      "24/11/19 12:23:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#Spark Session creation configured to interact with Kfka and MongoDB\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").\\\n",
    "config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-avro_2.12:3.0.0,org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "config(\"spark.mongodb.input.uri\",\"mongodb://docker_mongo_1:27017/twitter_db.tweets\").\\\n",
    "config(\"spark.mongodb.output.uri\",\"mongodb://docker_mongo_1:27017/twitter_db.tweets\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800977e3",
   "metadata": {
    "id": "394f0e2c"
   },
   "outputs": [],
   "source": [
    "#spark.read.json(\"reviews_Sports_and_Outdoors_5.json.gz\").show(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e8dbb3",
   "metadata": {
    "id": "0e7b9b96",
    "outputId": "716381d8-0374-4219-a493-615e0efae504"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "296337"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download dataset if not exists and read it as spark dataframe\n",
    "try:\n",
    "    df0 = spark.read.json(\"reviews_Sports_and_Outdoors_5.json.gz\")\n",
    "except Exception as e:\n",
    "    url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz\"\n",
    "    wget.download(url)\n",
    "    df0 = spark.read.json(\"reviews_Sports_and_Outdoors_5.json.gz\")\n",
    "\n",
    "df = df0.withColumn(\"text\",concat(col(\"summary\"), lit(\" \"),col(\"reviewText\")))\\\n",
    " .drop(\"helpful\")\\\n",
    " .drop(\"reviewerID\")\\\n",
    " .drop(\"reviewerName\")\\\n",
    " .drop(\"reviewTime\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9577fd62",
   "metadata": {
    "id": "111b66c2",
    "outputId": "a9c28e07-4cfd-418f-b54c-53ee86a7bc36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|           overall|\n",
      "+-------+------------------+\n",
      "|  count|            296337|\n",
      "|   mean| 4.393450699710128|\n",
      "| stddev|0.9869053992908551|\n",
      "|    min|               1.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"overall\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01be0d2a",
   "metadata": {
    "id": "91bdffd0",
    "outputId": "9927373e-5fef-4870-dbe4-f06362330b5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=================================================>      (66 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+\n",
      "|overall|label| count|\n",
      "+-------+-----+------+\n",
      "|    2.0|  0.0| 10204|\n",
      "|    5.0|  1.0|188208|\n",
      "|    1.0|  0.0|  9045|\n",
      "|    4.0|  1.0| 64809|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Bucketize data and create labels 0 if overall rating is in (1.0,2.0), otherwise 1\n",
    "df1 = df.filter(\"overall !=3\")\n",
    "\n",
    "splits = [-float(\"inf\"), 4.0, float(\"inf\")]\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"overall\", outputCol=\"label\")\n",
    "\n",
    "df2= bucketizer.transform(df1)\n",
    "\n",
    "df2.groupBy(\"overall\",\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2693a2",
   "metadata": {
    "id": "0bce2dc6",
    "outputId": "75c6f1e2-9115-40c8-f8e7-911c8d911763"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=================================================>      (66 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|19249|\n",
      "|  1.0|25224|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#take sample to create train and test dataset\n",
    "fractions = {1.0 : .1, 0.0 : 1.0}\n",
    "df3 = df2.stat.sampleBy(\"label\", fractions, 36)\n",
    "df3.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0259e68a",
   "metadata": {
    "id": "d27c2f55"
   },
   "outputs": [],
   "source": [
    "#Split data as 80-20% Train and Test dataset\n",
    "splitSeed = 5043\n",
    "trainingData, testData = df3.randomSplit([0.8, 0.2], splitSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c5a78e",
   "metadata": {
    "id": "09a57ba9"
   },
   "outputs": [],
   "source": [
    "#Tokenize \n",
    "tokenizer = RegexTokenizer(inputCol=\"text\",outputCol=\"reviewTokensUf\",pattern=\"\\\\s+|[,.()\\\"]\")\n",
    "\n",
    "remover = StopWordsRemover(stopWords=StopWordsRemover.loadDefaultStopWords(\"english\"),inputCol=\"reviewTokensUf\",outputCol=\"reviewTokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345f79fc",
   "metadata": {
    "id": "074e54d7"
   },
   "outputs": [],
   "source": [
    "#converts word documents to vectors of token counts\n",
    "cv = CountVectorizer(inputCol=\"reviewTokens\",outputCol=\"cv\",vocabSize=296337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd11b6fb",
   "metadata": {
    "id": "8d009b27"
   },
   "outputs": [],
   "source": [
    "#IDF model\n",
    "idf = IDF(inputCol=\"cv\",outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fad6a3ee",
   "metadata": {
    "id": "c753c97c"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=100,regParam=0.02,elasticNetParam=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "700c36dc",
   "metadata": {
    "id": "b49b029a"
   },
   "outputs": [],
   "source": [
    "#Creates a pipeline\n",
    "steps =  [tokenizer, remover, cv, idf,lr]\n",
    "pipeline = Pipeline(stages=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca1e01c4",
   "metadata": {
    "id": "d8b9c448"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 12:26:16 WARN DAGScheduler: Broadcasting large task binary with size 1969.4 KiB\n",
      "24/11/19 12:26:26 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:26 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/11/19 12:26:26 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/11/19 12:26:27 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:28 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:28 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:29 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:29 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:30 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:30 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:31 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:31 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:31 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:32 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:32 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:32 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:33 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:33 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:34 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:34 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:34 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:35 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:35 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:35 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:36 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:36 WARN BlockManager: Asked to remove block broadcast_77, which does not exist\n",
      "24/11/19 12:26:36 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:36 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:37 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:38 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:38 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:39 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:39 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:39 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:40 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:40 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:40 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:41 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:41 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:41 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:42 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:42 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:42 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:43 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:43 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:44 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:44 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:44 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:45 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:45 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:45 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:46 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:46 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:46 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:47 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:47 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:48 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:48 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:48 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:49 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:49 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:49 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:50 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:50 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:50 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:51 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:51 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:52 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:52 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:52 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:53 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:53 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:54 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:54 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:54 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:54 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:55 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:55 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:55 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:56 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:57 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:58 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:58 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:59 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:59 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:59 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:26:59 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:00 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:00 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:00 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:01 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:01 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:01 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:02 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:02 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:02 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:03 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:03 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n",
      "24/11/19 12:27:04 WARN DAGScheduler: Broadcasting large task binary with size 1968.6 KiB\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4711a246",
   "metadata": {
    "id": "c3ed06a6"
   },
   "outputs": [],
   "source": [
    "#collecting all metrics\n",
    "vocabulary = model.stages[2].vocabulary\n",
    "weights = model.stages[-1].coefficients.toArray()\n",
    "weights = [float(weight) for weight in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "365a58f4",
   "metadata": {
    "id": "b9b8a949"
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('word', StringType()),\n",
    "                     StructField('weight', FloatType())\n",
    "                     ])\n",
    "cdf = spark.createDataFrame(zip(vocabulary, weights), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80d34047",
   "metadata": {
    "id": "56346b17",
    "outputId": "4ab8aafe-8635-4531-b456-e25a748aa1b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 130:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     word|    weight|\n",
      "+---------+----------+\n",
      "|    great| 0.5876225|\n",
      "|   thoses|  0.325535|\n",
      "|  perfect|0.32343474|\n",
      "|     easy| 0.2615016|\n",
      "|   highly|0.25427502|\n",
      "|     love|0.23299988|\n",
      "|excellent|0.22146676|\n",
      "|     nice|0.21586789|\n",
      "|     good|0.20862874|\n",
      "|    works|0.20269535|\n",
      "+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cdf.orderBy(desc(\"weight\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32f2b2ba",
   "metadata": {
    "id": "0dd14bb1",
    "outputId": "963a9ba5-59d7-462a-e3e6-32f02593522b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         word|     weight|\n",
      "+-------------+-----------+\n",
      "|     returned|-0.38842562|\n",
      "|         poor|-0.33077022|\n",
      "|      useless|-0.30299458|\n",
      "|        waste|-0.27846226|\n",
      "|        broke|-0.26966578|\n",
      "|         junk| -0.2493974|\n",
      "|       return|-0.24831308|\n",
      "|disappointing|-0.22999014|\n",
      "|    returning|-0.21706156|\n",
      "| disappointed|-0.21414408|\n",
      "+-------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdf.orderBy(\"weight\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d742466",
   "metadata": {
    "id": "4a35d20a"
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0c3354b",
   "metadata": {
    "id": "86682040"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 12:27:11 WARN DAGScheduler: Broadcasting large task binary with size 1986.0 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()  \n",
    "areaUnderROC = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "880f21cc",
   "metadata": {
    "id": "7d624986",
    "outputId": "4cf3b71f-5198-45e3-be3d-bc570f222c77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 12:27:21 WARN DAGScheduler: Broadcasting large task binary with size 2003.5 KiB\n",
      "[Stage 141:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+--------------------+--------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|      asin|overall|          reviewText|             summary|unixReviewTime|                text|label|      reviewTokensUf|        reviewTokens|                  cv|            features|       rawPrediction|         probability|prediction|\n",
      "+----------+-------+--------------------+--------------------+--------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|7245456313|    1.0|I wish I would ha...|Defective - Be Ca...|    1354492800|Defective - Be Ca...|  0.0|[defective, -, be...|[defective, -, ca...|(71899,[0,11,15,1...|(71899,[0,11,15,1...|[1.99668098749145...|[0.88044816229074...|       0.0|\n",
      "|7245456313|    5.0|I bought this ban...|Great product, aw...|    1400112000|Great product, aw...|  1.0|[great, product, ...|[great, product, ...|(71899,[0,1,2,4,5...|(71899,[0,1,2,4,5...|[-3.3010460840682...|[0.03553531986812...|       1.0|\n",
      "|7245456313|    5.0|I used to be a pe...|GREAT product for...|    1304899200|GREAT product for...|  1.0|[great, product, ...|[great, product, ...|(71899,[1,8,16,20...|(71899,[1,8,16,20...|[-1.4400664803236...|[0.19153505391365...|       1.0|\n",
      "|7245456313|    5.0|My arms are burni...|Love Love Love th...|    1358985600|Love Love Love th...|  1.0|[love, love, love...|[love, love, love...|(71899,[6,10,13,3...|(71899,[6,10,13,3...|[-3.2229472289436...|[0.03831125139765...|       1.0|\n",
      "|B00000IURU|    5.0|Use this at pre s...|Toddlers love thi...|    1400803200|Toddlers love thi...|  1.0|[toddlers, love, ...|[toddlers, love, ...|(71899,[4,18,38,3...|(71899,[4,18,38,3...|[-0.6913907181625...|[0.33372377250877...|       1.0|\n",
      "|B00000J6JO|    1.0|As I write this r...|Very Cheaply made...|    1369699200|Very Cheaply made...|  0.0|[very, cheaply, m...|[cheaply, made, p...|(71899,[4,8,14,18...|(71899,[4,8,14,18...|[1.19648991575829...|[0.76789977056535...|       0.0|\n",
      "|B00000J6JO|    4.0|I saw a lot of ne...|Really good gift ...|    1401148800|Really good gift ...|  1.0|[really, good, gi...|[really, good, gi...|(71899,[0,2,3,4,5...|(71899,[0,2,3,4,5...|[-0.4114566537646...|[0.39856289439000...|       1.0|\n",
      "|B0000224UE|    5.0|I was given this ...|   Always by my side|    1361404800|Always by my side...|  1.0|[always, by, my, ...|[always, side, gi...|(71899,[0,4,5,6,1...|(71899,[0,4,5,6,1...|[0.07039114776984...|[0.51759052424809...|       0.0|\n",
      "|B0000224UE|    5.0|The victor inbox ...|Victorinox Multi-...|    1361923200|Victorinox Multi-...|  1.0|[victorinox, mult...|[victorinox, mult...|(71899,[6,14,16,1...|(71899,[6,14,16,1...|[-1.1410191572624...|[0.24213329163981...|       1.0|\n",
      "|B000030056|    1.0|Cheap product!  W...|       Cheap product|    1309564800|Cheap product Che...|  0.0|[cheap, product, ...|[cheap, product, ...|(71899,[4,8,14,17...|(71899,[4,8,14,17...|[1.16342300609821...|[0.76195413639682...|       0.0|\n",
      "|B00003CYPK|    5.0|Trac Ball is just...|One of the best b...|    1226188800|One of the best b...|  1.0|[one, of, the, be...|[one, best, backy...|(71899,[0,3,5,13,...|(71899,[0,3,5,13,...|[-3.1076942546118...|[0.04279098786357...|       1.0|\n",
      "|B00004NKIQ|    5.0|This net is great...|excellent net for...|    1341532800|excellent net for...|  1.0|[excellent, net, ...|[excellent, net, ...|(71899,[6,14,21,2...|(71899,[6,14,21,2...|[-2.1941902835827...|[0.10027341808034...|       1.0|\n",
      "|B00004SQM7|    2.0|This must be more...|          Didn't Fit|    1307404800|Didn't Fit This m...|  0.0|[didn't, fit, thi...|[fit, must, ideal...|(71899,[9,39,41,4...|(71899,[9,39,41,4...|[2.05943139774295...|[0.88689714564144...|       0.0|\n",
      "|B00004SQM9|    1.0|This lock jammed ...|            Not good|    1234656000|Not good This loc...|  0.0|[not, good, this,...|[good, lock, jamm...|(71899,[2,60,113,...|(71899,[2,60,113,...|[0.27827736694020...|[0.56912384699701...|       0.0|\n",
      "|B00004SQM9|    2.0|Works great on fi...|doesn't work well...|    1272326400|doesn't work well...|  0.0|[doesn't, work, w...|[work, well, leve...|(71899,[1,5,6,11,...|(71899,[1,5,6,11,...|[-0.9750220790220...|[0.27388062786153...|       1.0|\n",
      "|B00004SQM9|    4.0|This is a great a...|Works for multipl...|    1369180800|Works for multipl...|  1.0|[works, for, mult...|[works, multiple,...|(71899,[0,1,5,15,...|(71899,[0,1,5,15,...|[-1.4518081953470...|[0.18972343897225...|       1.0|\n",
      "|B00004SQM9|    5.0|I like the combin...|Very good trigger...|    1342051200|Very good trigger...|  1.0|[very, good, trig...|[good, trigger, l...|(71899,[2,3,26,44...|(71899,[2,3,26,44...|[-0.9615364235615...|[0.27657068276025...|       1.0|\n",
      "|B00004T1JW|    5.0|These bases are v...|Very nice set of ...|    1357084800|Very nice set of ...|  1.0|[very, nice, set,...|[nice, set, bases...|(71899,[6,13,15,1...|(71899,[6,13,15,1...|[-1.5148134864671...|[0.18022652998300...|       1.0|\n",
      "|B00004THDC|    4.0|Excellent optics....|    Excellent Optics|    1386288000|Excellent Optics ...|  1.0|[excellent, optic...|[excellent, optic...|(71899,[6,20,22,2...|(71899,[6,20,22,2...|[-2.9582846383313...|[0.04934641382990...|       1.0|\n",
      "|B00004TQ2P|    5.0|Good for kids and...|  Good for my nephew|    1382832000|Good for my nephe...|  1.0|[good, for, my, n...|[good, nephew, go...|(71899,[2,31,47,7...|(71899,[2,31,47,7...|[-0.5040760931476...|[0.37658325100325...|       1.0|\n",
      "+----------+-------+--------------------+--------------------+--------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9ed8438",
   "metadata": {
    "id": "8f3fb74b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 12:27:31 WARN DAGScheduler: Broadcasting large task binary with size 1983.3 KiB\n",
      "24/11/19 12:27:37 WARN DAGScheduler: Broadcasting large task binary with size 1983.3 KiB\n",
      "24/11/19 12:27:46 WARN DAGScheduler: Broadcasting large task binary with size 1983.3 KiB\n",
      "24/11/19 12:27:51 WARN DAGScheduler: Broadcasting large task binary with size 1983.3 KiB\n",
      "24/11/19 12:27:57 WARN DAGScheduler: Broadcasting large task binary with size 1983.5 KiB\n",
      "24/11/19 12:28:02 WARN DAGScheduler: Broadcasting large task binary with size 1983.5 KiB\n",
      "24/11/19 12:28:07 WARN DAGScheduler: Broadcasting large task binary with size 1983.5 KiB\n",
      "24/11/19 12:28:11 WARN DAGScheduler: Broadcasting large task binary with size 1983.5 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "lp = predictions.select(\"label\", \"prediction\")\n",
    "counttotal = predictions.count()\n",
    "correct = lp.filter(col(\"label\") == col(\"prediction\")).count()\n",
    "wrong = lp.filter(~(col(\"label\") == col(\"prediction\"))).count()\n",
    "ratioWrong = float(wrong) / float(counttotal)\n",
    "lp = predictions.select(  \"prediction\",\"label\")\n",
    "counttotal = float(predictions.count())\n",
    "correct = lp.filter(col(\"label\") == col(\"prediction\")).count()\n",
    "wrong = lp.filter(\"label != prediction\").count()\n",
    "ratioWrong=wrong/counttotal\n",
    "ratioCorrect=correct/counttotal\n",
    "trueneg =( lp.filter(col(\"label\") == 0.0).filter(col(\"label\") == col(\"prediction\")).count()) /counttotal\n",
    "truepos = (lp.filter(col(\"label\") == 1.0).filter(col(\"label\") == col(\"prediction\")).count())/counttotal\n",
    "falseneg = (lp.filter(col(\"label\") == 0.0).filter(~(col(\"label\") == col(\"prediction\"))).count())/counttotal\n",
    "falsepos = (lp.filter(col(\"label\") == 1.0).filter(~(col(\"label\") == col(\"prediction\"))).count())/counttotal\n",
    "\n",
    "precision= truepos / (truepos + falsepos)\n",
    "recall= truepos / (truepos + falseneg)\n",
    "#fmeasure= 2  precision  recall / (precision + recall)\n",
    "accuracy=(truepos + trueneg) / (truepos + trueneg + falsepos + falseneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b175af2",
   "metadata": {
    "id": "b7c3824f",
    "outputId": "632ee324-c3f8-4519-f7b0-be3dc85f63e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counttotal   : 9003.0\n",
      "correct      : 7776\n",
      "wrong        : 1227\n",
      "ratioWrong   : 0.13628790403198934\n",
      "ratioCorrect : 0.8637120959680107\n",
      "truen        : 0.3361101854937243\n",
      "truep        : 0.5276019104742864\n",
      "falsen       : 0.08863712095968011\n",
      "falsep       : 0.04765078307230923\n",
      "precision    : 0.9171654759606103\n",
      "recall       : 0.8561643835616438\n",
      "accuracy     : 0.8637120959680107\n"
     ]
    }
   ],
   "source": [
    "print('counttotal   :', counttotal     )\n",
    "print('correct      :', correct        )\n",
    "print('wrong        :', wrong          )\n",
    "print('ratioWrong   :', ratioWrong     )\n",
    "print('ratioCorrect :', ratioCorrect   )\n",
    "print('truen        :', trueneg          )\n",
    "print('truep        :', truepos          )\n",
    "print('falsen       :', falseneg         )\n",
    "print('falsep       :', falsepos         )\n",
    "print('precision    :', precision      )\n",
    "print('recall       :', recall         )\n",
    "#print('fmeasure     :', fmeasure       )\n",
    "print('accuracy     :', accuracy       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90cc62c5",
   "metadata": {
    "id": "ea2f523d",
    "outputId": "c892f587-d3eb-4439-dd16-177052de438e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 12:28:17 WARN DAGScheduler: Broadcasting large task binary with size 1996.3 KiB\n",
      "[Stage 162:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+----------+\n",
      "|             summary|        reviewTokens|overall|prediction|\n",
      "+--------------------+--------------------+-------+----------+\n",
      "|Buyer Beware - Yo...|[buyer, beware, -...|    2.0|       0.0|\n",
      "|Awful Phone and T...|[awful, phone, te...|    1.0|       0.0|\n",
      "|DO NOT BUY HERE I...|[buy, need, custo...|    1.0|       0.0|\n",
      "|                JUNK|[junk, well, rece...|    1.0|       0.0|\n",
      "|Poor 3-9x40 Hamme...|[poor, 3-9x40, ha...|    1.0|       0.0|\n",
      "+--------------------+--------------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.filter(col(\"prediction\") == 0.0)\\\n",
    ".select(\"summary\",\"reviewTokens\",\"overall\",\"prediction\")\\\n",
    ".orderBy(desc(\"rawPrediction\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b91050d7",
   "metadata": {
    "id": "67f67c23",
    "outputId": "10f5971b-77a4-495c-8207-403a13154f5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 12:28:24 WARN DAGScheduler: Broadcasting large task binary with size 1996.2 KiB\n",
      "[Stage 163:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+----------+\n",
      "|             summary|        reviewTokens|overall|prediction|\n",
      "+--------------------+--------------------+-------+----------+\n",
      "|My DROID Story an...|[droid, story, co...|    5.0|       1.0|\n",
      "| great trucker phone|[great, trucker, ...|    5.0|       1.0|\n",
      "|    Favorite EDC Bag|[favorite, edc, b...|    4.0|       1.0|\n",
      "|One of My Favorit...|[one, favorites!!...|    4.0|       1.0|\n",
      "|Best Hopper I've ...|[best, hopper, us...|    4.0|       1.0|\n",
      "+--------------------+--------------------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.filter(col(\"prediction\")== 1.0)\\\n",
    ".select(\"summary\",\"reviewTokens\",\"overall\",\"prediction\")\\\n",
    ".orderBy(\"rawPrediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf256bf7",
   "metadata": {
    "id": "46797c12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 12:28:32 WARN TaskSetManager: Stage 168 contains a task of very large size (1385 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/19 12:28:34 WARN TaskSetManager: Stage 171 contains a task of very large size (1153 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "dir = \"sentiment/\"\n",
    "model.write().overwrite().save(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3d7194c",
   "metadata": {
    "id": "33b356fa"
   },
   "outputs": [],
   "source": [
    "dir = \"sentiment/\"\n",
    "model = PipelineModel.load(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d205b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 12:32:05.814470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-19 12:32:06.792969: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-19 12:32:06.976863: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-19 12:32:06.976969: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-19 12:32:10.993076: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-19 12:32:10.993431: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-19 12:32:10.993451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-11-19 12:32:23.564201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-11-19 12:32:23.565322: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-11-19 12:32:23.565386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d70576e64267): /proc/driver/nvidia/version does not exist\n",
      "2024-11-19 12:32:23.572137: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,329,473\n",
      "Trainable params: 1,329,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1109/1109 [==============================] - 164s 144ms/step - loss: 0.3426 - accuracy: 0.8516 - val_loss: 0.2596 - val_accuracy: 0.8954\n",
      "Epoch 2/5\n",
      "1109/1109 [==============================] - 151s 136ms/step - loss: 0.2027 - accuracy: 0.9219 - val_loss: 0.2688 - val_accuracy: 0.8925\n",
      "Epoch 3/5\n",
      "1109/1109 [==============================] - 142s 128ms/step - loss: 0.1473 - accuracy: 0.9463 - val_loss: 0.3001 - val_accuracy: 0.8897\n",
      "Epoch 4/5\n",
      "1109/1109 [==============================] - 149s 135ms/step - loss: 0.1037 - accuracy: 0.9632 - val_loss: 0.2933 - val_accuracy: 0.8945\n",
      "Epoch 5/5\n",
      "1109/1109 [==============================] - 141s 127ms/step - loss: 0.0733 - accuracy: 0.9749 - val_loss: 0.3468 - val_accuracy: 0.8904\n",
      "282/282 [==============================] - 11s 38ms/step - loss: 0.3468 - accuracy: 0.8904\n",
      "Test Loss: 0.3467799127101898\n",
      "Test Accuracy: 0.8903698921203613\n"
     ]
    }
   ],
   "source": [
    "# Chuyển đổi dữ liệu từ Spark DataFrame sang Pandas\n",
    "train_df = trainingData.select(\"text\", \"label\").toPandas()\n",
    "test_df = testData.select(\"text\", \"label\").toPandas()\n",
    "\n",
    "# Tokenizer và Padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Cấu hình tokenizer\n",
    "max_vocab_size = 10000  # Số lượng từ tối đa\n",
    "max_seq_length = 100    # Chiều dài chuỗi tối đa\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "# Biến đổi văn bản sang chuỗi số\n",
    "X_train = tokenizer.texts_to_sequences(train_df['text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Padding để có độ dài chuỗi bằng nhau\n",
    "X_train = pad_sequences(X_train, maxlen=max_seq_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_seq_length)\n",
    "\n",
    "# Nhãn\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Xây dựng mô hình LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_vocab_size, output_dim=128, input_length=max_seq_length),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Phân loại nhị phân\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42adc094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 12s 39ms/step\n",
      "                                                Text  Actual Label  \\\n",
      "0  Defective - Be Careful! I wish I would have ta...           0.0   \n",
      "1  Great product, awesome warranty, amazing custo...           1.0   \n",
      "2  GREAT product for the money I used to be a per...           1.0   \n",
      "3  Love Love Love the bands My arms are burning a...           1.0   \n",
      "4  Toddlers love this thing Use this at pre schoo...           1.0   \n",
      "5  Very Cheaply made product. As I write this rev...           0.0   \n",
      "6  Really good gift - more for fun than for \"prac...           1.0   \n",
      "7  Always by my side I was given this tool some 1...           1.0   \n",
      "8  Victorinox Multi-Tool The victor inbox Multi-T...           1.0   \n",
      "9  Cheap product Cheap product!  Within two days ...           0.0   \n",
      "\n",
      "   Predicted Label  Prediction Probability  \n",
      "0                0                0.001845  \n",
      "1                1                0.883534  \n",
      "2                1                0.999785  \n",
      "3                1                0.998045  \n",
      "4                1                0.899518  \n",
      "5                0                0.000957  \n",
      "6                0                0.247919  \n",
      "7                1                0.995414  \n",
      "8                1                0.987274  \n",
      "9                0                0.002864  \n"
     ]
    }
   ],
   "source": [
    "# Dự đoán kết quả trên tập test\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Chuyển đổi xác suất thành nhãn dự đoán (0 hoặc 1)\n",
    "predicted_labels = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "# In một số dự đoán mẫu\n",
    "import pandas as pd\n",
    "\n",
    "# Gộp văn bản, nhãn thực tế và dự đoán vào một DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Text': test_df['text'].values[:10],           # Lấy 10 văn bản đầu tiên\n",
    "    'Actual Label': y_test[:10],\n",
    "    'Predicted Label': predicted_labels[:10].flatten(),\n",
    "    'Prediction Probability': predictions[:10].flatten()\n",
    "})\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"sentiment/\"\n",
    "model.write().overwrite().save(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"sentiment/\"\n",
    "model = PipelineModel.load(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1561d5c2",
   "metadata": {
    "id": "05251f93",
    "outputId": "f1e2ce9d-c8a2-4089-f51e-efb6035c56ba"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o803.load.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=docker_mongo_1:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: docker_mongo_1}, caused by {java.net.UnknownHostException: docker_mongo_1}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator$lzycompute(MongoRDD.scala:221)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator(MongoRDD.scala:221)\n\tat com.mongodb.spark.sql.MongoInferSchema$.apply(MongoInferSchema.scala:68)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:97)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51/2399674026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mongo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o803.load.\n: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=docker_mongo_1:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: docker_mongo_1}, caused by {java.net.UnknownHostException: docker_mongo_1}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:182)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.executeCommand(MongoDatabaseImpl.java:194)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:163)\n\tat com.mongodb.client.internal.MongoDatabaseImpl.runCommand(MongoDatabaseImpl.java:158)\n\tat com.mongodb.spark.MongoConnector.$anonfun$hasSampleAggregateOperator$1(MongoConnector.scala:234)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.hasSampleAggregateOperator(MongoConnector.scala:234)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator$lzycompute(MongoRDD.scala:221)\n\tat com.mongodb.spark.rdd.MongoRDD.hasSampleAggregateOperator(MongoRDD.scala:221)\n\tat com.mongodb.spark.sql.MongoInferSchema$.apply(MongoInferSchema.scala:68)\n\tat com.mongodb.spark.sql.DefaultSource.constructRelation(DefaultSource.scala:97)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:339)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"mongo\").load()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf833a9",
   "metadata": {
    "id": "fc22ffb8",
    "outputId": "bcfc57d0-d04a-4e88-b453-c00665c792eb"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\").load().select(\"timestamp\",\"text\")\n",
    "splits = [-float(\"inf\"), 0, float(\"inf\")]\n",
    "#bucketizer = Bucketizer(inputCol=\"timestamp_ms\",outputCol=\"sentiment\",splits=splits)\n",
    "\n",
    "#df5= bucketizer.transform(df)\n",
    "predictions = model.transform(df)\n",
    "predictions.select('text','prediction').show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentimentanalyzer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
